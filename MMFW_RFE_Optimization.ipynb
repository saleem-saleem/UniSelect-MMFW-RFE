# Import required libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_selection import SelectKBest, chi2, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score
from sklearn.datasets import load_iris

# Function to select top-k features using Chi-Square test
def SelectTopKFeaturesUsingChiSquare(X, y, k):
    chi2_selector = SelectKBest(chi2, k=k)
    chi2_selector.fit(X, y)
    selected_features = chi2_selector.get_support(indices=True)
    return selected_features

# Function to evaluate the classifier performance (Accuracy, F-score)
def EvaluateClassifierPerformance(classifier, X, y):
    y_pred = classifier.predict(X)
    accuracy = accuracy_score(y, y_pred)
    f_score = f1_score(y, y_pred, average='weighted')
    return accuracy, f_score

# Function for Recursive Feature Elimination (RFE)
def ReducedRecursiveFeatureElimination(X, y, n, classifier):
    rfe_selector = RFE(classifier, n_features_to_select=n)
    rfe_selector.fit(X, y)
    selected_features = rfe_selector.get_support(indices=True)
    return selected_features

# Function to perform Cross-validated Recursive Feature Elimination
def CrossValidatedRecursiveFeatureElimination(X, y, n, classifier, cv=5):
    rfe_selector = RFE(classifier, n_features_to_select=n)
    scores = cross_val_score(rfe_selector, X, y, cv=cv, scoring='accuracy')
    return np.mean(scores)

# Main MMFW-RFE function
def MMFW_RFE(X, y, k, classifier, n):
    # Step 1: Feature selection using Chi-Square
    FS1 = SelectTopKFeaturesUsingChiSquare(X, y, k)
    
    # Step 2: Feature selection using Feature Importance (Random Forest in this case)
    classifier.fit(X, y)
    feature_importance = classifier.feature_importances_
    FS2 = np.argsort(feature_importance)[-k:]  # Select top-k features based on importance
    
    # Step 3: Union of FS1 and FS2
    FS = np.union1d(FS1, FS2)
    k1 = len(FS)
    
    if k1 < k:
        print("Warning: The number of selected features is less than k. Proceeding with k1 = k.")
        k1 = k
    elif k1 > k:
        print("Warning: The number of selected features is greater than k. Proceeding with k1 = k.")
        FS = FS[:k]
    
    # Step 4: Perform Reduced Recursive Feature Elimination (RRFE)
    RRFE_PA = ReducedRecursiveFeatureElimination(X[:, FS], y, n, classifier)
    
    # Step 5: Evaluate performance of RRFE (Accuracy, F-score)
    RRFE_Accuracy, RRFE_Fscore = EvaluateClassifierPerformance(classifier, X[:, RRFE_PA], y)
    
    # Step 6: Cross-Validated Recursive Feature Elimination (CV-RRFE)
    CV_RRFE_FSOI = CrossValidatedRecursiveFeatureElimination(X[:, FS], y, n, classifier)
    
    # Step 7: Evaluate performance of CV-RRFE
    classifier.fit(X[:, FS], y)
    CV_RRFE_Accuracy, CV_RRFE_Fscore = EvaluateClassifierPerformance(classifier, X[:, FS], y)
    
    # Step 8: Compare RRFE and CV-RRFE performance and select the best features
    if RRFE_Fscore >= CV_RRFE_Fscore:
        PA = RRFE_PA
        FSOI = RRFE_Fscore
        Accuracy = RRFE_Accuracy
        F_score = RRFE_Fscore
    else:
        PA = FS[:k]  # Take the top k features from the union FS
        FSOI = CV_RRFE_FSOI
        Accuracy = CV_RRFE_Accuracy
        F_score = CV_RRFE_Fscore
    
    return PA, FSOI, Accuracy, F_score
