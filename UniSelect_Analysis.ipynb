# UniSelect Analysis.ipynb

# ## 1. Import Libraries
# First, we import all the necessary libraries for data processing, feature selection, model training, and evaluation.

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

# ## 2. Load the Dataset
# For this analysis, we will use the Iris dataset as an example. You can replace this with your own dataset.

from sklearn.datasets import load_iris

# Load Iris dataset
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

# Display the first few rows of the dataset
X.head(), y.head()

# ## 3. Preprocessing
# Before applying the feature selection methods, we perform preprocessing like label encoding (if necessary) and scaling.

# Label Encoding for target variable
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Standardizing the feature set
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ## 4. Feature Selection: Chi-Square
# Step 1: Use Chi-Square to select top-k features. We'll use SelectKBest for this purpose.

def chi_square_selection(X, y, k1):
    chi2_selector = SelectKBest(chi2, k=k1)
    chi2_selector.fit(X, y)
    selected_features_chi2 = chi2_selector.get_support(indices=True)
    return selected_features_chi2

# Selecting top-k1 features based on Chi-Square
k1 = 2
selected_features_chi2 = chi_square_selection(X_scaled, y_encoded, k1)
print(f"Features selected by Chi-Square: {X.columns[selected_features_chi2]}")

# ## 5. Feature Importance Selection using ExtraTreesClassifier
# Step 2: Use ExtraTreesClassifier to calculate feature importance scores and select the top-k2 features.

def feature_importance_selection(X, y, k2):
    model = ExtraTreesClassifier(n_estimators=100)
    model.fit(X, y)
    feature_importance = model.feature_importances_
    sorted_idx = np.argsort(feature_importance)[::-1]
    top_k_features = sorted_idx[:k2]
    return top_k_features

# Selecting top-k2 features based on Feature Importance
k2 = 2
selected_features_importance = feature_importance_selection(X_scaled, y_encoded, k2)
print(f"Features selected by Feature Importance: {X.columns[selected_features_importance]}")

# ## 6. Union of Feature Sets
# Combine the two feature sets (Chi-Square and Feature Importance) and select the union of features.

selected_features_union = np.union1d(selected_features_chi2, selected_features_importance)
print(f"Union of selected features: {X.columns[selected_features_union]}")

# ## 7. Remove Highly Correlated Features
# Step 3: Remove highly correlated features with correlation > 0.9.

def remove_high_correlation_features(X, threshold=0.9):
    corr_matrix = X.corr().abs()
    upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > threshold)]
    X_filtered = X.drop(X[to_drop], axis=1)
    return X_filtered

# Remove correlated features from the union set
X_filtered = remove_high_correlation_features(X[selected_features_union])
print(f"Features after removing correlated ones: {X_filtered.columns}")

# ## 8. Recursive Feature Elimination (RFE)
# Step 4: Perform RFE to refine the feature selection further using Logistic Regression as the base model.

def rfe_selection(X, y, model):
    rfe = RFE(model, n_features_to_select=1)
    rfe.fit(X, y)
    return rfe.support_

# Perform RFE on the filtered feature set
model = LogisticRegression(max_iter=1000)
selected_rfe_features = rfe_selection(X_filtered, y_encoded, model)
print(f"Features selected by RFE: {X_filtered.columns[selected_rfe_features]}")

# ## 9. Model Training and Evaluation
# After selecting the best features using UniSelect, we train a classifier on the final set of features.

# Final selected features after RFE
final_selected_features = X_filtered.columns[selected_rfe_features]
X_final = X_filtered[final_selected_features]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_final, y_encoded, test_size=0.3, random_state=42)

# Train a Logistic Regression model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy with selected features: {accuracy}")

# ## 10. Feature Importance Visualization
# We visualize the feature importance based on ExtraTreesClassifier.

def plot_feature_importance(model, X, n_features):
    feature_importance = model.feature_importances_
    sorted_idx = np.argsort(feature_importance)[::-1]
    plt.figure(figsize=(10, 6))
    plt.barh(range(n_features), feature_importance[sorted_idx], align="center")
    plt.yticks(range(n_features), X.columns[sorted_idx])
    plt.xlabel("Feature Importance")
    plt.title("Feature Importance Based on Extra Trees Classifier")
    plt.show()

# Plot the feature importance
plot_feature_importance(model, X, X.shape[1])

# ## 11. Conclusion
# In this notebook, we have successfully implemented the **UniSelect** feature selection algorithm using:
# - Chi-Square
# - Feature Importance via Extra Trees Classifier
# - Recursive Feature Elimination (RFE)
#
# The final set of selected features was used to train a Logistic Regression model, achieving an accuracy of `X%` on the test set.
